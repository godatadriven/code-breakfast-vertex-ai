{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf9e0930-9f3a-4008-a214-7e4e325ab40d",
   "metadata": {},
   "source": [
    "## Running locally (on the VM)\n",
    "\n",
    "Before running our model in a pipeline, we first would like to run through the whole process locally and see if/how it works. \n",
    "\n",
    "We'll start off by copying our training dataset to the VM so it's easier to work with. This is easy to do with the `gsutil` command line utility, which allows us to copy files to/from Google Cloud Storage buckets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e55a97-b2db-4bb7-bc99-23ff1edc3d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy training data locally (run once).\n",
    "! mkdir -p ~/tmp/fashion\n",
    "! gsutil cp -r gs://gdd-cb-vertex-fashion-inputs/train ~/tmp/fashion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ffc8c3",
   "metadata": {},
   "source": [
    "**Exercise 1**\n",
    "\n",
    "Now we've managed to get a copy of our training data, we'd like to start training the model. \n",
    "\n",
    "1. Read through `model.py` and see if you can find the function responsible for training the model.\n",
    "2. Apply the function to our dataset and see if you are able to train a model instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836c049e-dd1c-491e-addc-07ded754bec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train an instance of our model.\n",
    "\n",
    "model = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26bef7c",
   "metadata": {},
   "source": [
    "**Exercise 2**\n",
    "\n",
    "To check the accuracy of our model, we'd like to evaluate it on some test data.\n",
    "\n",
    "1. Download our test dataset (`gs://gdd-cb-vertex-fashion-inputs/test`) in the same fashion as we did for the training data.\n",
    "2. Check `model.py` and see if you can find the function responsible for evaluating the model.\n",
    "3. Apply the function to our test dataset and print the returned metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6152e66-4848-4608-b1fa-4bd0b4eefce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Copy test data to the VM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03db5b5-a6f0-4810-961a-21f35b4104c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Evaluate our model and print the metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc36201f",
   "metadata": {},
   "source": [
    "**Exercise 3**\n",
    "\n",
    "Finally, lets test our prediction functionality by generating some predictions for unlabeled data.\n",
    "\n",
    "1. Download our validation dataset (`gs://gdd-cb-vertex-fashion-inputs/validation`) in the same fashion as we did for the training data.\n",
    "2. Check `model.py` and see if you can find the function responsible for generating predictions.\n",
    "3. Apply the function to our validation dataset and print the returned predictions."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "poetry-kernel",
   "name": "common-cpu.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m91"
  },
  "interpreter": {
   "hash": "f25696975e9fe87f4a49fc82a1153ef630435f4d436d60b49f4c90b848f15e05"
  },
  "kernelspec": {
   "display_name": "Poetry",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
