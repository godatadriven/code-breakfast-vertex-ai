{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be273a47-5ac5-48e7-b30e-9fdd6f1d887d",
   "metadata": {},
   "source": [
    "## Running in Vertex Pipelines\n",
    "\n",
    "To make our model training more reproducible, we would like to run it in an automated pipeline that clearly defines the different steps we need to take (e.g. training and evaluating the model) and captures any produced artifacts (e.g. the trained model). In the Google Cloud, we can use Vertex Pipelines for this purpose. \n",
    "\n",
    "Vertex Pipelines allows you to define pipelines as a graph of containerized tasks, in which each task performs a specific step needed to train/evaluate/deploy a model. In this notebook we'll define a pipeline interactively and see how we can use it to run our model on top of Vertex's serverless compute infrastructure.\n",
    "\n",
    "First, let's do some quick setup to set some variables we'll need later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99568b41-54e8-49a7-b589-ad40d486a06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a folder for storing our component/pipeline artifacts.\n",
    "! mkdir -p _artifacts\n",
    "\n",
    "# Set up some required variables.\n",
    "GCP_REGION = \"europe-west3\"\n",
    "\n",
    "# Enter your name here. We'll use this to tag your unique\n",
    "# Docker image to avoid clashing with other people.\n",
    "USER_NAME = input(\"Your user name:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3932ea",
   "metadata": {},
   "source": [
    "Next, to be able to run our model code as a containerized task in our Vertex Pipeline, we need to build it into a Docker image and push it to a registry that Vertex AI can access.\n",
    "\n",
    "You can use the following command to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c236f400-a59d-46c2-8caa-98a62265c07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "! make -C ../ USER_NAME=$USER_NAME docker-push"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba7d2c3",
   "metadata": {},
   "source": [
    "**Exercise 1 - Building the Docker image**\n",
    "\n",
    "* Check out the Makefile to see what this command does.\n",
    "* Inspect the Dockerfile to see what's inside our Docker image. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12603e5c",
   "metadata": {},
   "source": [
    "Now we've created our Docker image, we'll start building our pipeline. \n",
    "\n",
    "Vertex Pipelines can be defined using the [Kubeflow Pipeline SDK](https://www.kubeflow.org/docs/components/pipelines/sdk/). In this SDK, tasks are typically defined using components, which reference a Docker image and define the operations that will be run in the image. These tasks can be combined using their inputs and outputs to define the overall pipeline.\n",
    "\n",
    "The easiest way to create a component in Python is using the `@component` decorator, which converts a Python function into a pipeline component:\n",
    "\n",
    "```\n",
    "@component(\n",
    "    base_image=\"my-image\",\n",
    "    output_component_file=\"_artifacts/train.yaml\",\n",
    ")\n",
    "def train(train_data_path: str, model: Output[Model]) -> None:\n",
    "    ...\n",
    "```\n",
    "\n",
    "This effectively tasks the code inside the function (`...`) and ensures the code will be run inside the referenced container when the task is executed. \n",
    "\n",
    "The component also takes input parameters allowing you to define inputs/outputs for the component (marked by `Input` or `Output` types) or other extra parameters. Note that adding typing for your parameters is crucial when defining a component, as Kubeflow uses these types when compiling your component.\n",
    "\n",
    "For more information on building Python-based components see [here](https://www.kubeflow.org/docs/components/pipelines/sdk/python-function-components/). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfde2d9",
   "metadata": {},
   "source": [
    "**Exercise 2 - Building and running our first pipeline**\n",
    "\n",
    "Let's build and run our first pipeline. Below we've provided the skeleton of a pipeline that defines a `train` task and an (empty) `evalute` task. \n",
    "\n",
    "1. Read through the code and see if you understand what it does.\n",
    "    * How is the `train` component defined? What does it do?\n",
    "    * How is the pipeline defined? What steps does it include?\n",
    "2. Run the cell to make sure the pipeline is defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7909875b-f647-4670-a135-4e026d4d4746",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, NamedTuple\n",
    "\n",
    "import kfp\n",
    "from kfp import components\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import (\n",
    "    component,\n",
    "    Input,\n",
    "    InputPath,\n",
    "    OutputPath,\n",
    "    Output,\n",
    "    Dataset,\n",
    "    Metrics,\n",
    "    Model\n",
    ")\n",
    "\n",
    "@component(\n",
    "    base_image=f\"{GCP_REGION}-docker.pkg.dev/gdd-cb-vertex/docker/fancy-fashion-{USER_NAME}\",\n",
    "    output_component_file=\"_artifacts/train.yaml\",\n",
    ")\n",
    "def train(train_data_path: str, model: Output[Model]) -> None:\n",
    "    \"\"\"Trains the model on the given dataset.\"\"\"\n",
    "    \n",
    "    from pathlib import Path\n",
    "    import joblib\n",
    "    \n",
    "    from fancy_fashion.model import train_model\n",
    "    from fancy_fashion.util import local_gcs_path\n",
    "    \n",
    "    trained_model = train_model(local_gcs_path(train_data_path))\n",
    "\n",
    "    model_dir = Path(model.path)\n",
    "    model_dir.mkdir(parents=True, exist_ok=True)\n",
    "    joblib.dump(trained_model, model_dir / \"model.pkl\")\n",
    "\n",
    "    \n",
    "@component(\n",
    "    base_image=f\"{GCP_REGION}-docker.pkg.dev/gdd-cb-vertex/docker/fancy-fashion-{USER_NAME}\",\n",
    "    output_component_file=\"_artifacts/evaluate.yaml\",\n",
    ")\n",
    "def evaluate(\n",
    "    test_data_path: str, model: InputPath(\"Model\"), metrics: Output[Metrics]\n",
    ") -> NamedTuple(\"EvalModelOutput\", [(\"roc\", float)]):\n",
    "    # TODO: Implement the actual evaluation.\n",
    "    #       Tip: we can use the evaluate_model function from our package.\n",
    "    metrics.log_metric(\"roc\", 0.9)\n",
    "\n",
    "    \n",
    "@kfp.dsl.pipeline(name=\"fancy-fashion-julian\")\n",
    "def pipeline(train_path: str):\n",
    "    train_task = train(train_path)\n",
    "    \n",
    "    # TODO: Add an evaluate task that uses the evaluate component above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f0a800",
   "metadata": {},
   "source": [
    "3. Now we've defined the pipeline, let's compile it using the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e35fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline,\n",
    "    package_path=\"_artifacts/pipeline.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce440ada",
   "metadata": {},
   "source": [
    "4. Finally, let's submit the compiled pipeline by creating and submitting a Pipeline job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f94aef-51a0-46b0-a2b6-2254a22a480f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud.aiplatform.pipeline_jobs import PipelineJob\n",
    "\n",
    "job = PipelineJob(\n",
    "    display_name=f\"fancy-fashion-{USER_NAME}\",\n",
    "    enable_caching=False,\n",
    "    template_path=\"_artifacts/pipeline.json\",\n",
    "    parameter_values={\n",
    "        \"train_path\": \"gs://gdd-cb-vertex-fashion-inputs/train\"\n",
    "    },\n",
    "    pipeline_root=f\"gs://gdd-cb-vertex-fashion-artifacts/pipelines\",\n",
    "    location=GCP_REGION,\n",
    ")\n",
    "\n",
    "job.run(\n",
    "    service_account=f\"vmd-fashion@gdd-cb-vertex.iam.gserviceaccount.com\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bf182f",
   "metadata": {},
   "source": [
    "5. Follow the pipeline run by clicking on the printed link. Does it finish successfully?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b7904a",
   "metadata": {},
   "source": [
    "**Exercise 3 - Extending the pipeline**\n",
    "\n",
    "Now we have an idea of how Kubeflow Pipelines are defined, lets see if we can add some extra functionality to the pipeline.\n",
    "\n",
    "1. Extend the pipeline by implementing the `evaluate` component to call our model evaluation function (defined in `model.py`). \n",
    "2. Add an extra step `predict` that generates model predictions using our prediction function. See if you can register the generated predictions as a `Dataset` artifact. (Tip: you can do so by passing an `Output` to your component function)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33f109f",
   "metadata": {},
   "source": [
    "**Exercise 4 - Validating your predictions**\n",
    "\n",
    "1. Check the predictions you generated against our validation set using the `validate_predictions` function from `validation.py`.\n",
    "2. Is your model as accurate as you expected? If not, do you have any idea why?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5a6f92",
   "metadata": {},
   "source": [
    "**Bonus exercises**\n",
    "\n",
    "Should you finish early, try exploring the following:\n",
    "\n",
    "* Can you find the pipeline runs and the generated artifacts in the console?\n",
    "* Where are the artifacts stored in GCP?\n",
    "* Can you retrieve these metadata details programatically? What about the model artifact path in GCS? (Tip: check the `get_pipeline_df` function in the `google.cloud.aiplatform` package and the `MetadataServiceClient` in the `google.cloud.aiplatform_v1` package).\n",
    "* How can you tell Kubeflow to use a GPU instance for running a specific task?\n",
    "* Can you deploy the trained model as a REST API endpoint?"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "poetry-kernel",
   "name": "common-cpu.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m91"
  },
  "kernelspec": {
   "display_name": "Poetry",
   "language": "python",
   "name": "poetry-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
